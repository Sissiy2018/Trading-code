{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c1b911b-0244-4b5d-b7c2-b361ca8d1bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Historical Price Data with ADV Filter using LSEG Data Packages\n",
    "\n",
    "This script fetches historical price and volume data, then filters stocks\n",
    "based on 3-month Average Daily Volume (ADV) >= 5M USD.\n",
    "\n",
    "ADV formula: ADV_t = (1/60) * Σ(USD_Volume_{t-i}) for i=1 to 60\n",
    "\"\"\"\n",
    "\n",
    "import lseg.data as ld\n",
    "import pandas as pd\n",
    "from typing import Tuple, List\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import json\n",
    "\n",
    "from pandas.tseries.holiday import USFederalHolidayCalendar\n",
    "from pandas.tseries.offsets import CustomBusinessDay\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a283c77-835e-4698-b2f1-e02c37fa1989",
   "metadata": {},
   "outputs": [],
   "source": [
    "ld.open_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8026e69-2ace-4097-a470-dd3d0571e4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "russel = ld.get_data(\"0#.RUA\",fields = 'TR.CommonName')\n",
    "russel.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb4b808-a5ae-4273-a490-12ebfbb236b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "rics = russel[\"Instrument\"].astype(str).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c653c4e-b6e5-4f6b-a538-916c1f321558",
   "metadata": {},
   "outputs": [],
   "source": [
    "end_date = datetime.now().strftime('%Y-%m-%d')\n",
    "start_date = '2026-02-12'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "545cae97-cdd5-4604-954c-b63f111c1daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_historical_data_price(rics, start_date, end_date, fields=None, batch_size=50):\n",
    "    \"\"\"\n",
    "    Fetch historical price and volume data in batches (single-core)\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    rics : list\n",
    "        List of Reuters Instrument Codes (RICs)\n",
    "    start_date : str\n",
    "        Start date in 'YYYY-MM-DD' format\n",
    "    end_date : str\n",
    "        End date in 'YYYY-MM-DD' format\n",
    "    fields : list, optional\n",
    "        Fields to retrieve. Default includes price and volume fields.\n",
    "    batch_size : int, optional\n",
    "        Number of RICs to process per batch (default: 50)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        Historical data with multi-index (Date, RIC)\n",
    "    \"\"\"\n",
    "    if fields is None:\n",
    "        fields = [\n",
    "            'TR.PriceClose',           # Closing price\n",
    "            #'TR.PriceOpen',            # Opening price\n",
    "            #'TR.PriceHigh',            # High price\n",
    "            #'TR.PriceLow',             # Low price\n",
    "            'TR.Volume',               # Trading volume (shares)\n",
    "            'TR.PriceClose.currency',   # Currency for price\n",
    "            'TR.TotalReturn1D',\n",
    "            #'TR.TRBCEconomicSector' # static object process later\n",
    "            \n",
    "        ]\n",
    "    \n",
    "    total_rics = len(rics)\n",
    "    num_batches = (total_rics + batch_size - 1) // batch_size\n",
    "    \n",
    "    print(f\"\\nFetching data for {total_rics} instruments...\")\n",
    "    print(f\"Date range: {end_date}\")\n",
    "    print(f\"Processing in {num_batches} batches of ~{batch_size} RICs each (single-core)\\n\")\n",
    "    \n",
    "    all_data = []\n",
    "    failed_batches = []\n",
    "    \n",
    "    try:\n",
    "        \n",
    "        for batch_num in range(num_batches):\n",
    "            # Get batch of RICs\n",
    "            batch_start = batch_num * batch_size\n",
    "            batch_end = min((batch_num + 1) * batch_size, total_rics)\n",
    "            batch_rics = rics[batch_start:batch_end]\n",
    "\n",
    "            \n",
    "            try:\n",
    "                # Fetch batch data\n",
    "                df_batch = ld.get_history(\n",
    "                    universe=batch_rics,\n",
    "                    fields=fields,\n",
    "                    start=start_date,\n",
    "                    end=end_date,\n",
    "                    interval='daily',\n",
    "                    #count = look_back_days,\n",
    "                    parameters = {'Curn': 'USD'}\n",
    "                )\n",
    "                \n",
    "                if not df_batch.empty:\n",
    "                    df_batch = df_batch.stack(level=0)   # stack RICs\n",
    "                    all_data.append(df_batch)\n",
    "                    print(\"✓\")\n",
    "                else:\n",
    "                    print(\"✗ (no data)\")\n",
    "                \n",
    "\n",
    "\n",
    "                progress = (batch_end / total_rics) * 100\n",
    "                print(f\"Progress: {progress:5.1f}% | Batch {batch_num+1}/{num_batches}s\")\n",
    "                        \n",
    "            except Exception as batch_error:\n",
    "                failed_batches.append(batch_num + 1)\n",
    "                print(f\"\\n⚠ Warning: Batch {batch_num+1} failed: {batch_error}\")\n",
    "\n",
    "        \n",
    "        # Combine all batches\n",
    "        if all_data:\n",
    "            df_combined = pd.concat(all_data)\n",
    "            df_combined.index.names = [\"Date\", \"RIC\"]\n",
    "            \n",
    "            #print(f\"\\n✓ Retrieved {len(df_combined):,} data points in {elapsed:.1f}s\")\n",
    "            #print(f\"  Unique instruments: {df_combined['Instrument'].nunique()}\")\n",
    "            #print(f\"  Date range: {df_combined['Date'].min()} to {df_combined['Date'].max()}\")\n",
    "            \n",
    "            if failed_batches:\n",
    "                print(f\"  ⚠ Failed batches: {failed_batches}\")\n",
    "            \n",
    "            return df_combined\n",
    "        else:\n",
    "            print(f\"\\n✗ No data retrieved\")\n",
    "            return None\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"\\n✗ Error fetching historical data: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "781e1348-af17-451b-b3a8-667877c1d77f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"rics_20260214_ADVfiltered.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    kept_rics = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "036f66de-5faf-4df5-ad9a-a77f1602df18",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_historical_data_price(rics, start_date, end_date, fields = None)\n",
    "\n",
    "df_final = df.reset_index()\n",
    "df_final.to_csv(\"lseg_historyprice_data_now_to_20260212.csv\", index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42f95f6-bfdb-46d1-b914-102623ace95c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_historical_data_price(kept_rics, start_date, end_date, fields = None)\n",
    "\n",
    "df_final = df.reset_index()\n",
    "df_final.to_csv(\"lseg_historyprice_data_now_to_20260212_ADVfiltered.csv\", index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e6aa09-3643-473d-bf0c-3d30035e259f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_historical_data_pe(rics, start_date, end_date, fields=None, batch_size=50):\n",
    "    \"\"\"\n",
    "    Fetch historical price and volume data in batches (single-core)\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    rics : list\n",
    "        List of Reuters Instrument Codes (RICs)\n",
    "    start_date : str\n",
    "        Start date in 'YYYY-MM-DD' format\n",
    "    end_date : str\n",
    "        End date in 'YYYY-MM-DD' format\n",
    "    fields : list, optional\n",
    "        Fields to retrieve. Default includes price and volume fields.\n",
    "    batch_size : int, optional\n",
    "        Number of RICs to process per batch (default: 50)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        Historical data with multi-index (Date, RIC)\n",
    "    \"\"\"\n",
    "    if fields is None:\n",
    "        fields = [\n",
    "            #'TR.PriceClose',           # Closing price\n",
    "            #'TR.PriceOpen',            # Opening price\n",
    "            #'TR.PriceHigh',            # High price\n",
    "            #'TR.PriceLow',             # Low price\n",
    "            #'TR.Volume',               # Trading volume (shares)\n",
    "            #'TR.PriceClose.currency',   # Currency for price\n",
    "            #'TR.TotalReturn1D',\n",
    "            #'TR.TRBCEconomicSector' # static object process later\n",
    "            \"TR.PE\",\n",
    "            \n",
    "        ]\n",
    "    \n",
    "    total_rics = len(rics)\n",
    "    num_batches = (total_rics + batch_size - 1) // batch_size\n",
    "    \n",
    "    print(f\"\\nFetching data for {total_rics} instruments...\")\n",
    "    print(f\"Date range: {end_date}\")\n",
    "    print(f\"Processing in {num_batches} batches of ~{batch_size} RICs each (single-core)\\n\")\n",
    "    \n",
    "    all_data = []\n",
    "    failed_batches = []\n",
    "    \n",
    "    try:\n",
    "        \n",
    "        for batch_num in range(num_batches):\n",
    "            # Get batch of RICs\n",
    "            batch_start = batch_num * batch_size\n",
    "            batch_end = min((batch_num + 1) * batch_size, total_rics)\n",
    "            batch_rics = rics[batch_start:batch_end]\n",
    "\n",
    "            \n",
    "            try:\n",
    "                # Fetch batch data\n",
    "                df_batch = ld.get_history(\n",
    "                    universe=batch_rics,\n",
    "                    fields=fields,\n",
    "                    start=start_date,\n",
    "                    end=end_date,\n",
    "                    interval='daily',\n",
    "                    #count = look_back_days,\n",
    "                    parameters = {'Curn': 'USD'}\n",
    "                )\n",
    "                \n",
    "                if not df_batch.empty:\n",
    "                    df_batch = df_batch.stack(level=0)   # stack RICs\n",
    "                    all_data.append(df_batch)\n",
    "                    print(\"✓\")\n",
    "                else:\n",
    "                    print(\"✗ (no data)\")\n",
    "                \n",
    "\n",
    "\n",
    "                progress = (batch_end / total_rics) * 100\n",
    "                print(f\"Progress: {progress:5.1f}% | Batch {batch_num+1}/{num_batches}s\")\n",
    "                        \n",
    "            except Exception as batch_error:\n",
    "                failed_batches.append(batch_num + 1)\n",
    "                print(f\"\\n⚠ Warning: Batch {batch_num+1} failed: {batch_error}\")\n",
    "\n",
    "        \n",
    "        # Combine all batches\n",
    "        if all_data:\n",
    "            df_combined = pd.concat(all_data)\n",
    "            df_combined.index.names = [\"Date\", \"RIC\"]\n",
    "            \n",
    "            # Rename column to a clean title\n",
    "            df_combined.columns = [\"Price to Earning\"]\n",
    "            \n",
    "            #print(f\"\\n✓ Retrieved {len(df_combined):,} data points in {elapsed:.1f}s\")\n",
    "            #print(f\"  Unique instruments: {df_combined['Instrument'].nunique()}\")\n",
    "            #print(f\"  Date range: {df_combined['Date'].min()} to {df_combined['Date'].max()}\")\n",
    "            \n",
    "            if failed_batches:\n",
    "                print(f\"  ⚠ Failed batches: {failed_batches}\")\n",
    "            \n",
    "            return df_combined\n",
    "        else:\n",
    "            print(f\"\\n✗ No data retrieved\")\n",
    "            return None\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"\\n✗ Error fetching historical data: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed7c285-b73a-43da-af10-aef8d372cca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_historical_data_pe(rics, start_date, end_date, fields = None)\n",
    "\n",
    "\n",
    "if isinstance(df, pd.Series):\n",
    "    df = df.to_frame(name=\"Price to Earning\")\n",
    "\n",
    "    \n",
    "df_final = df.reset_index()\n",
    "\n",
    "\n",
    "df_final.to_csv(\"lseg_Price-Earning_data_now_to_20260212.csv\", index = False, encoding=\"utf-8-sig\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edde814f-9f95-49f0-a938-c6180ece6e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_historical_data_pe(kept_rics, start_date, end_date, fields = None)\n",
    "\n",
    "\n",
    "if isinstance(df, pd.Series):\n",
    "    df = df.to_frame(name=\"Price to Earning\")\n",
    "\n",
    "    \n",
    "df_final = df.reset_index()\n",
    "\n",
    "\n",
    "df_final.to_csv(\"lseg_Price-Earning_data_now_to_20260212_ADVfiltered.csv\", index = False, encoding=\"utf-8-sig\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "396c2cf6-fd5e-436b-b7eb-de4145a66b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_historical_data(\".SPX\", start_date, end_date, fields = None)\n",
    "\n",
    "df_final.to_csv(\"lseg_historyprice_S&P500_now_to_20260212.csv\", index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "745b56b2-0667-4f25-a8e1-3042d05facbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_usd_volume(df):\n",
    "    \"\"\"\n",
    "    Calculate USD trading volume for each (Date, RIC)\n",
    "\n",
    "    USD_Volume = Volume * Price Close * FX_Rate (if not USD)\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        Must contain:\n",
    "        ['Volume', 'Price Close', 'Currency']\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        With added 'USD_Volume' column\n",
    "    \"\"\"\n",
    "    print(\"\\nCalculating USD volume...\")\n",
    "\n",
    "    df = df.copy()\n",
    "\n",
    "    # Sanity check\n",
    "    required_cols = {\"Volume\", \"Price Close\", \"Currency\"}\n",
    "    missing = required_cols - set(df.columns)\n",
    "    if missing:\n",
    "        raise ValueError(f\"Missing required columns: {missing}\")\n",
    "\n",
    "    # Base calculation (assume USD)\n",
    "    df[\"USD_Volume\"] = df[\"Volume\"] * df[\"Price Close\"]\n",
    "\n",
    "    # Identify non-USD rows\n",
    "    non_usd = df[\"Currency\"] != \"USD\"\n",
    "    non_usd_df = df.loc[df[\"Currency\"].ne(\"USD\")].copy()\n",
    "\n",
    "\n",
    "    if non_usd.any():\n",
    "        print(f\"⚠ {non_usd.sum()} rows are non-USD (FX rate = 1.0 placeholder)\")\n",
    "        # Placeholder FX rate\n",
    "        df.loc[non_usd, \"USD_Volume\"] *= 1.0\n",
    "\n",
    "    return df, non_usd_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd1a61b-9074-4085-8726-a859e5c48500",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_adv(df, lookback_days=75, min_coverage=0.8):\n",
    "    print(f\"\\nCalculating {lookback_days}-day ADV for each stock...\")\n",
    "\n",
    "    adv_records = []\n",
    "\n",
    "    for ric, g in df.groupby(level=\"RIC\"):\n",
    "        g = g.sort_index(level=\"Date\")\n",
    "\n",
    "        recent = g.tail(lookback_days)\n",
    "        n_obs = recent[\"USD_Volume\"].notna().sum()\n",
    "\n",
    "        if n_obs >= lookback_days * min_coverage:\n",
    "            adv_records.append({\n",
    "                \"RIC\": ric,\n",
    "                \"ADV_USD\": recent[\"USD_Volume\"].mean(),\n",
    "                \"Days_Used\": n_obs,\n",
    "                \"Latest_Date\": recent.index.get_level_values(\"Date\")[-1]\n",
    "            })\n",
    "        else:\n",
    "            print(f\"  Warning: Insufficient data for {ric} ({n_obs} days)\")\n",
    "\n",
    "    return pd.DataFrame(adv_records)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a68c9ab5-7632-47a7-8ceb-db4d041a3fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_single_stock_limits(\n",
    "    adv_summary: pd.DataFrame,\n",
    "    threshold_usd: float = 5_000_000,\n",
    "    adv_col: str = \"ADV_USD\",\n",
    "    ric_col: str = \"RIC\",\n",
    "    pct_of_adv: float = 0.025,\n",
    "    max_usd_cap: float = 2_000_000,\n",
    "    limit_col: str = \"Daily_Limit_USD\",\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame, List[str]]:\n",
    "    \"\"\"\n",
    "    Step 1: Screen by ADV >= threshold_usd (e.g. 5M).\n",
    "    Step 2: For kept names, compute Daily_Limit_USD = min(pct_of_adv * ADV, max_usd_cap).\n",
    "\n",
    "    Returns:\n",
    "        filtered_adv: kept rows with Daily_Limit_USD added\n",
    "        excluded_adv: excluded rows (no daily limit computed)\n",
    "        kept_rics: list of kept RICs\n",
    "    \"\"\"\n",
    "    df = adv_summary.copy()\n",
    "\n",
    "    required = {adv_col, ric_col}\n",
    "    missing = required - set(df.columns)\n",
    "    if missing:\n",
    "        raise ValueError(f\"Missing required columns: {missing}\")\n",
    "\n",
    "    df[adv_col] = pd.to_numeric(df[adv_col], errors=\"coerce\")\n",
    "\n",
    "    # Step 1: ADV screen\n",
    "    mask_keep = df[adv_col] >= threshold_usd\n",
    "    filtered_adv = df.loc[mask_keep].copy()\n",
    "    excluded_adv = df.loc[~mask_keep].copy()\n",
    "    kept_rics = filtered_adv[ric_col].dropna().tolist()\n",
    "\n",
    "    # Step 2: compute daily trading limit only for kept\n",
    "    filtered_adv[limit_col] = (filtered_adv[adv_col] * pct_of_adv).clip(upper=max_usd_cap)\n",
    "\n",
    "\n",
    "    return filtered_adv, excluded_adv, kept_rics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30622e82-91dc-406f-951b-5adaa9d8a542",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
